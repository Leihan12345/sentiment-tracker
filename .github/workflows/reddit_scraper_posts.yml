name: Reddit Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"  # every hour

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Cache the virtualenv instead of just pip's download cache
      - name: Cache venv
        id: cache-venv
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-py311-${{ hashFiles('requirements.txt') }}

      # Only build/install when cache misses or requirements.txt changes
      - name: Install dependencies
        if: steps.cache-venv.outputs.cache-hit != 'true'
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        env:
          CLIENT_ID: ${{ secrets.CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
          USER_AGENT: ${{ secrets.USER_AGENT }}
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
        run: |
          source .venv/bin/activate
          python -m app.services.reddit_service_post
